---
title: "S&DS 220 Report Template"
author: "Braeden Cullen"
date: "April 21st, 2024"
output:
  pdf_document:
    toc: true
    number_sections: true
    toc_depth: 3
urlcolor: blue
---

```{r setup, include=FALSE}
# set options for the R markdown document
knitr::opts_chunk$set(echo = TRUE,      ## show or suppress the code
                      include = TRUE ,      ## show or suppress the output
                      message = FALSE,      ## omit messages generated by code
                      warning = FALSE,      ## omit warnings generated by code
                      comment = NA,         ## removes the ## from in front of outputs
                      fig.align = "center", ## centers all figures
                      fig.height = 5,     ## set the default height
                      fig.weight = 5      ## set the default width
)

# this is a good place to include the library calls for the packages you use
library(tidyverse)
library(caret)
library(plotly)
library(pROC)
library(e1071)
library(randomForest)
```


# Introduction

# Problem Statement

- a non-technical description of the problem you are trying to solve or the question you are trying to answer, and why you are trying to answer that question

10% of all people over the age of 65 have Alzheimer's disease, and as many as 50% of people over 85 have it. The number of people with the disease doubles every 5 years beyond age 65. Alzheimers is a progressive neurodegenerative disease that affects millions of people worldwide. It is the most common cause of dementia and is characterized by memory loss, cognitive decline, and behavioral changes. The disease is currently incurable, but early detection can help slow its progression and improve the quality of life for affected individuals. Someone in the United States develops Alzheimer's disease every 65 seconds, and it is the sixth leading cause of death in the country. The cost of caring for people with Alzheimer's and other dementias is estimated to be $305 billion in 2020, and this number is expected to rise to $1.1 trillion by 2050. The disease is a major public health concern, and there is an urgent need for effective diagnostic tools and treatments. Early stage Alzheimer's detection, in particular, is crucial for developing interventions that can slow or stop the progression of the disease. The goal of this project is to develop a machine learning model that can predict whether a person has Alzheimer's disease based on demographic and clinical data. The model will be trained on a dataset of patients with and without Alzheimer's disease and will be evaluated on its ability to accurately classify new patients as either having or not having the disease. The model will be used to identify risk factors for Alzheimer's disease and to develop a predictive tool that can help clinicians diagnose the disease in its early stages.

# Hypothesis

- a non-technical description of the hypothesis you are testing

Early-stage alzehimers can be effectively detected through graphological analysis of patient handwriting data. The most effective indicators of early-stage alzherimers will likely be related to the size, shape, and speed of the handwriting, as well as the number of pen lifts and pen strokes. Using a series of metrics, we can determine which features are most indicative of early-stage alzheimers, and use these features to develop a predictive model that can accurately classify patients as either having or not having the disease.

# Data Overview

- a non-technical description of the data, where it came from, and what it contains, including the predictors, the outcome, and the observations

This project focuses on using graphological analysis of patient handwriting data, provided through the DARWIN dataset, to determine indicators of early-stage Alzheimers. This dataset contains data from 174 patients, including 89 patients with Alzheimer's disease and 85 healthy controls. The data includes wether the patient has Alzheimer's disease or not, as well as 451 features carefully collected during the participant handwriting process. These features include the number of pen lifts and the number of pen strokes, as well as various measures of the size, shape, and speed of the handwriting. 

# Data exploration and visualization

- This section will give an overview of the data.  It should include descriptive statistics and visualizations of the raw data.  Reveal to the reader any interesting relationships in the data, and if you are doing multiple regression, convince the reader that the predictors are related to the outcome. Visualizations are one of the most powerful ways to communicate information to the reader, so it is important to spend time producing clear, descriptive, eye-catching visualizations. 
- again, this should be nontechnical

# Methodology

- a non-technical description of what kind of analysis you did and how to interpret the results of the model

# Results

- a non-technical description of the results of the model and main takeaways.


# Modeling/Analysis

Describe the statistical inference/hypothesis testing/regression model(s) used and the analysis that was performed.  Discuss 

- Any assumptions that are made
- The observations (the rows of the data), the predictors (non-outcome columns of the data), and the outcome (one of the column of the data)
- Interpret the results of the model
    - If regression:
        - What the coefficients mean and how this is related to your problem
        - Appropriate measures of the performance of the model, such as adjusted $R^2$
        - How easy/hard it is to interpret the results and explain them to either a technical or non-technical audience. For example, do the coefficients have the expected sign? Are the sizes (magnitudes) of the coefficients reasonable, and can you put them in real world terms?
    - For other kinds of analysis, what you give is highly dependent on the type of analysis you do. But in general, talk about assumptions, if they are appropriate, how they might not be appropriate, and why you chose this type of analysis. 
- Whether or not you think the model is appropriate for this kind of data, and why.


# Visualization and interpretation of the results

Create visualizations of the results, focusing on visualizations that 

- help describe aspects of the results that have real-world interpretation 
- help the reader understand how the model addresses the problem you are studying. 

**Visualizations are one of the most powerful ways to communicate information to the reader, so it is important to spend time producing clear, descriptive, eye-catching visualizations.**

Discuss the results of the model or models you chose, and describe how they are related to the problem statement or question that you were trying to answer in the project.  

If you build multiple models or perform multiple types of analysis, compare the measures of performance and the ease of interpretability across models or types of analysis, stating which model or models performed best, and which model or models were most interpretable.  Finally, decide which model or type of analysis is best for your particular problem based on some combination of performance and interpretability.

# Conclusions and recommendations

A few sentences stating conclusions, recommendations, and ideas for future work and improvements.


# Code

Guiding question: Can we predict the existence of Alzheimer's disease based on patient handwriting data?

## Data Preprocessing

```{r}
# Load the data
data <- read.csv("darwin.csv")

# Print Dimmensions of Face Data
print(dim(data))

# Check for missing values
if(sum(is.na(data)) != 0) {
  print("There are missing values in the data")
}

# Check for duplicates
if(sum(duplicated(data)) != 0) {
  print("There are duplicates in the data")
}

# Convert diagnosis into a binary value, P = 1 HC = 0
data$class <- ifelse(data$class == "P", 1, 0)

# Indicies, we will be limiting our analysis to the first 100 features
x1_idx = 2;
x2_idx = 100; 
diagnosis_idx = 452; 

data_condensed <- data %>% select(x1_idx:x2_idx, diagnosis_idx)

# Create train / test splits, save trustworthiness data
set.seed(123)
train_index <- sample(1:nrow(data_condensed), 0.7*nrow(data_condensed))
train_data <- data_condensed[train_index,]
test_data <- data_condensed[-train_index,]

# Save diagnosis before removing for training, last row of train_data / test_data
train_data_outcomes <- train_data %>% select(x2_idx - x1_idx + 2)
test_data_outcomes <- test_data %>% select(x2_idx - x1_idx + 2)

# Remove diagnosis category
train_data_removed <- train_data %>% select(1:(x2_idx - x1_idx + 1))
test_data_removed <- test_data %>% select(1:(x2_idx - x1_idx + 1))

# Isolate the data to only utilize the very first handwriting task
train_data <- train_data_removed %>% select(1:18)
test_data <- test_data_removed %>% select(1:18)
```
## Exploration of Data

```{r}
# Summary of the data
summary(train_data)

# Identify the distribution of each feature in the data mapped alongside the diagnosis
for (i in 1:ncol(train_data)) 
{
  hist(train_data[,i], col = "salmon", main = colnames(train_data)[i], xlab = colnames(train_data)[i])
  xfit<-seq(min(train_data[,i]),max(train_data[,i]),length=40)
  yfit<-dnorm(xfit,mean=mean(train_data[,i]),sd=sd(train_data[,i]))
  yfit <- yfit*diff(h$mids[1:2])*length(train_data[,i])
  plot(xfit, yfit, type="l", col="salmon", lwd=2, add=TRUE)
  boxplot(train_data[,i], col = "salmon", main = colnames(train_data)[i], xlab = colnames(train_data)[i])
}

# Density plots of each feature by diagnosis
for (i in 1:ncol(train_data)) 
{
  plot(density(train_data[train_data_outcomes == 0, i]), col = "salmon", main = colnames(train_data)[i], xlab = colnames(train_data)[i])
  lines(density(train_data[train_data_outcomes == 1, i]), col = "blue")
}

# Measure the correlation between each feature and the diagnosis
correlation_matrix <- cor(train_data, unlist(train_data_outcomes))

# Visualize the correlation matrix
plot_ly(y = colnames(train_data), z = correlation_matrix, type = "heatmap")

# By examining the variation, we can get a rough estimate of the importance of each feature, the variance must be scaled for each feature
variance <- apply(train_data, 2, var)

# Scale the variance for visualization
variance <- variance / max(variance)

# Visualize the variance of each feature
plot_ly(x = colnames(train_data), y = variance, type = "bar", marker = list(color = "salmon"))

# Perform Chi-Squared Test for all features, store to a list
for(i in 1:ncol(train_data)) 
{
  feature_storage <- list()
  chi_test <- chisq.test(train_data[,i], train_data$class)
  # store the name of the feature
  feature_storage[[i]] <- colnames(train_data)[i]
  print(chi_test)
}

# Print the top 10 features with the highest variance
top_10_features <- sort(variance, decreasing = TRUE)[1:10]
top_10_features

```




## Variable Selection

```{r}
# Linear Regression Model
set.seed(42)
lm_model <- lm(unlist(train_data_outcomes) ~ ., data=train_data)

# Linear Regression Model: Hand-Picked Features
selected_features <- c("pressure_var1", "total_time1", "air_time1", "paper_time1", "max_y_extension1", "max_x_extension1", "pressure_mean1", "gmrt_in_air1", "mean_gmrt1", "gmrt_on_paper1")
lm_model_selected <- lm(unlist(train_data_outcomes) ~ ., data=train_data[,selected_features])

# Model Evaluation All Features
summary(lm_model)
predictions_train <- predict(lm_model, newdata=train_data)
predictions_test <- predict(lm_model, newdata=test_data)
predictions_list <- unname(as.list(as.data.frame(train_data)))
train_data_outcomes_numeric <- as.numeric(unlist(train_data_outcomes))

# Model Evaluation Selected Features
summary(lm_model_selected)
predictions_train_selected <- predict(lm_model_selected, newdata=train_data)
predictions_test_selected <- predict(lm_model_selected, newdata=test_data)
predictions_list_selected <- unname(as.list(as.data.frame(train_data)))
train_data_outcomes_numeric_selected <- as.numeric(unlist(train_data_outcomes))

# All Features Model MSE
mse_linear_regression_all <- mean((unlist(test_data_outcomes) - as.numeric(predictions_test))^2)
print(mse_linear_regression_all)

# Selected Features Model MSE
mse_linear_regression_selected <- mean((unlist(test_data_outcomes) - as.numeric(predictions_test_selected))^2)
print(mse_linear_regression_selected)

# Find the statistically significant coefficents of the model
significant_coefficients <- summary(lm_model)$coefficients[summary(lm_model)$coefficients[,4] < 0.05,]
print(significant_coefficients)

# Calculate the adjusted R^2 value of Selected Feature Model
adjusted_r_squared <- summary(lm_model)$adj.r.squared
print(adjusted_r_squared)

# Create a correlation test for the linear regression model
cor.test(unlist(test_data_outcomes), predictions_test_selected)

# Visualizing the Selected Feature Model  
plot(lm_model_selected)

```

## Modelling
```{r}
# Begin SVM Model
svm_model <- svm(unlist(train_data_outcomes) ~ ., data=train_data_removed, kernel="linear", cost=10)

# Model Evaluation
predictions_test_svm <- predict(svm_model, newdata=test_data_removed)

# Calculate the accuracy of the model
accuracy_train_svm <- sum(predictions_test_svm == unlist(test_data_outcomes)) / length(predictions_test_svm)

# Create a confusion matrix
confusion_matrix <- table(predictions_test_svm, unlist(test_data_outcomes))

# Calculate the sensitivity and specificity of the model
sensitivity_svm <- confusion_matrix[2,2] / (confusion_matrix[2,2] + confusion_matrix[2,1])
specificity_svm <- confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,2])

# Calculate MSE
mse_svm <- mean((unlist(test_data_outcomes) - as.numeric(predictions_test_svm))^2)
mse_svm

# Store the model results
model_results <- data.frame(model = "SVM", accuracy = accuracy_train_svm, sensitivity = sensitivity_svm, specificity = specificity_svm)

# Begin Random Forest Model
rf_model <- randomForest(unlist(train_data_outcomes) ~ ., data=train_data_removed, ntree=100)

# Model Evaluation
predictions_test_rf <- predict(rf_model, newdata=test_data_removed)

# Calculate the accuracy of the models
accuracy_train_rf <- sum(predictions_test_rf == unlist(test_data_outcomes)) / length(predictions_test_rf)

# Create a confusion matrix
confusion_matrix_rf <- table(predictions_test_rf, unlist(test_data_outcomes))

# Calculate the sensitivity and specificity of the model
sensitivity_rf <- confusion_matrix[2,2] / (confusion_matrix[2,2] + confusion_matrix[2,1])
specificity_rf <- confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,2])

# Calculate MSE
mse_rf <- mean((unlist(test_data_outcomes) - as.numeric(predictions_test_rf))^2)
mse_rf

# Store the model results
model_results <- rbind(model_results, data.frame(model = "Random Forest", accuracy = accuracy_train_rf, sensitivity = sensitivity_rf, specificity = specificity_rf))

```
